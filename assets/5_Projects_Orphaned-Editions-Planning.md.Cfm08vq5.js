import{_ as t,c as a,o,ag as r}from"./chunks/framework.BRQrZDXk.js";const u=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"5_Projects/Orphaned-Editions-Planning.md","filePath":"5_Projects/Orphaned-Editions-Planning.md"}'),i={name:"5_Projects/Orphaned-Editions-Planning.md"};function s(n,e,h,d,l,c){return o(),a("div",null,e[0]||(e[0]=[r(`<h2 id="_5-million-orphans" tabindex="-1">5 Million Orphans <a class="header-anchor" href="#_5-million-orphans" aria-label="Permalink to &quot;5 Million Orphans&quot;">​</a></h2><p>&quot;Orphaned Editions&quot; are Open Library book records that only have an edition item (OL..M) and no corresponding work (OL..W). Search results list works, and the main landing page for a book is a work page. One work can have many editions. Editions without works break the data model and cause various problems with search, links, and syncing data. There is a hack to make search mostly work with these orphans where the edition is indexed as if it is a work. No current process is creating new orphans, but there exist 5 million from the early days of Open Library. An ongoing task has been to find a way to reduce and fix these completely.</p><p><img src="https://docs.google.com/spreadsheets/d/e/2PACX-1vTEIOrqppehDt3GFfGwVsqefoUWkB3syOJHZLakNsf2alWkW28s8G6DaqXf58ZXiOgZZV6oIn5f78Fz/pubchart?oid=1153405837&amp;format=image" alt="orphaned editions graph"></p><h2 id="feb-2019-update" tabindex="-1">Feb 2019 Update <a class="header-anchor" href="#feb-2019-update" aria-label="Permalink to &quot;Feb 2019 Update&quot;">​</a></h2><p>Of the remaining 5M orphaned editions:</p><ul><li>IA sourced orphans: 70381 These are not getting fixed by re-importing due to various issues where the importer is preventing the import to avoid non-book items. We may want to enable an override to relax the restrictions to fix the missing work?</li></ul><p>These remaining items appear to be blocked from re-importing due to:</p><p>Microfiche example: &#39;cihm_01524&#39;</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>b&#39;{&quot;error_code&quot;: &quot;bad-repub-state&quot;, &quot;success&quot;: false, &quot;error&quot;: &quot;Prohibited Item&quot;}&#39;</span></span></code></pre></div><p>These are microfiche scans, there are 3746 cihm_ items in this category</p><p>Other examples:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>  veteransadminist0383unit: {&quot;error_code&quot;: &quot;bad-repub-state&quot;, &quot;success&quot;: false, &quot;error&quot;: &quot;Prohibited Item&quot;}</span></span>
<span class="line"><span>  colonnade12andiuoft: {&quot;error_code&quot;: &quot;item-is-serial&quot;, &quot;success&quot;: false, &quot;error&quot;: &quot;Invalid item&quot;}</span></span>
<span class="line"><span>  deecclesiasticb00schogoog: {&quot;error_code&quot;: &quot;noindex-true&quot;, &quot;success&quot;: false, &quot;error&quot;: &quot;Prohibited Item&quot;}</span></span>
<span class="line"><span>  americanforests04natiuoft: {&quot;error_code&quot;: &quot;invalid-ia-identifier&quot;, &quot;success&quot;: false, &quot;error&quot;: &quot;Invalid item&quot;}</span></span></code></pre></div><p>6277 of these ocaid items have source_records from MARC files, so these can be re-imported that way. <strong>[DONE]</strong></p><ul><li>Editions with source records in metadata: 695K These are books that were imported from library MARC records in the early days of Open Library, before Works were added. Re-importing these by passing their source to the <code>/api/import/ia</code> endpoint will generate the Work. This is currently in progress by a bot task. <strong>[DONE FEB 2019]</strong></li></ul><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>18 marc:unc_catalog_marc</span></span>
<span class="line"><span>48 marc:bpl_marc</span></span>
<span class="line"><span>75 marc:wfm_bk_marc</span></span>
<span class="line"><span>140 marc:hollis_marc</span></span>
<span class="line"><span>422 marc:marc_binghamton_univ</span></span>
<span class="line"><span>884 marc:talis_openlibrary_contribution</span></span>
<span class="line"><span>1761 marc:bcl_marc</span></span>
<span class="line"><span>1819 marc:marc_laurentian</span></span>
<span class="line"><span>2152 marc:CollingswoodLibraryMarcDump10-27-2008</span></span>
<span class="line"><span>5179 marc:marc_miami_univ_ohio</span></span>
<span class="line"><span>8037 marc:marc_cca</span></span>
<span class="line"><span>9066 marc:SanFranPL</span></span>
<span class="line"><span>12091 marc:marc_university_of_toronto</span></span>
<span class="line"><span>25167 marc:marc_ithaca_college</span></span>
<span class="line"><span>33450 marc:marc_western_washington_univ</span></span>
<span class="line"><span>46041 marc:marc_oregon_summit_records</span></span>
<span class="line"><span>87970 marc:marc_upei</span></span>
<span class="line"><span>141227 marc:marc_records_scriblio_net</span></span>
<span class="line"><span>634103 marc:marc_loc_updates</span></span></code></pre></div><ul><li><p>1.7M items do not have ISBNs -- these seem to be very early imports from MARC records that are not captured in metadata, but are available in the history / comments. These can be fixed by checking the source from the history, and then re-importing. I am currently running a script to discover the source MARC records for these 1.7M items.</p></li><li><p>2.5M do not have a MARC source or an ocaid, but have ISBNs.</p></li></ul><p>Of those, most seem to be imported very early on from Amazon. Some of these will also be from library MARC records where the source is not captured in metadata, and these can be distinguished by running a similar check on the comments for the initial import comment. Unfortunately, the existing Amazon lookup will NOT trigger the creation of a Work if only an edition is found. This needs to be fixed to allow these ISBN / Amazon source editions to generate works.</p><h2 id="_2018-update" tabindex="-1">2018 Update <a class="header-anchor" href="#_2018-update" aria-label="Permalink to &quot;2018 Update&quot;">​</a></h2><p>The information in the sections below is still accurate, but has been difficult to act upon at scale.</p><p>Recently the Import process has been updated to allow reliable re-imports of existing books from individual, or bulk, MARC record sources. Part of this revised process adds works to matched editions if they don&#39;t already exist. This means orphaned editions can be fixed by reimporting from their MARC source, or matching archive.org items by using an <a href="https://github.com/internetarchive/openlibrary/wiki/Endpoints#importing" target="_blank" rel="noreferrer">import endpoint</a>.</p><p>This is a huge win, and will allow the editions-without-works problem to be solved finally, once we re-process all the data.</p><h3 id="new-process-drawing-on-the-plans-below" tabindex="-1">New process drawing on the plans below <a class="header-anchor" href="#new-process-drawing-on-the-plans-below" aria-label="Permalink to &quot;New process drawing on the plans below&quot;">​</a></h3><p>For each category identified below, in priority order, filter the orphan edition records from the OL db dumps, determine the original source from <code>source_records</code> and perform the appropriate <a href="https://github.com/internetarchive/openlibrary/wiki/Endpoints#importing" target="_blank" rel="noreferrer">re-import request</a>.</p><h2 id="planning" tabindex="-1">Planning <a class="header-anchor" href="#planning" aria-label="Permalink to &quot;Planning&quot;">​</a></h2><p>Query to extract all orphans from OL edition dump (25M records). [<strong>DONE!</strong>] Extract 5M orphans.</p><h3 id="gather-statistics-divide-into-groups" tabindex="-1">Gather statistics, divide into groups. <a class="header-anchor" href="#gather-statistics-divide-into-groups" aria-label="Permalink to &quot;Gather statistics, divide into groups.&quot;">​</a></h3><h3 id="focus-on-orphans-with-ocaids-because" tabindex="-1">Focus on orphans with OCAIDs because: <a class="header-anchor" href="#focus-on-orphans-with-ocaids-because" aria-label="Permalink to &quot;Focus on orphans with OCAIDs because:&quot;">​</a></h3><p>these are the records that are borrowable / readable and their discoverability is adversely affected by not being associated to works. Has implications for search engine indexing, and the v2 availability API.</p><p><strong>First priority:</strong> Duplicate OCAID orphans. These have two issues -- not associated with any work, and are duplicate records which should be merged down. These can be matched with existing records that have works and turned into redirects, or grouped together and have a single work created for the best record. <strong>COMPLETED</strong></p><p><strong>Second stage</strong> Merge down remaining duplicate OCAID works. There are as of August 2017 60,786 identified groups of duplicate works, so at least twice that many records. These groupings need further checking to determine the safest way to merge the records down.</p><table tabindex="0"><thead><tr><th>Groupings</th><th style="text-align:right;">Number of OCAIDs</th><th style="text-align:right;">Total number of works/editions</th></tr></thead><tbody><tr><td>Doubled</td><td style="text-align:right;">47,467</td><td style="text-align:right;">94,934</td></tr><tr><td>Tripled</td><td style="text-align:right;">4,761</td><td style="text-align:right;">14,283</td></tr><tr><td>Quadrupled</td><td style="text-align:right;">666</td><td style="text-align:right;">2,664</td></tr><tr><td>5+ dupes</td><td style="text-align:right;">58</td><td style="text-align:right;"><em>(at least)</em> 290</td></tr><tr><td><strong>TOTAL</strong></td><td style="text-align:right;">52,952</td><td style="text-align:right;">112,171</td></tr></tbody></table><p><strong>Third stage</strong> Clean out any bad records, spam / bad data / non-book items in the list of orphans to reduce identification and merge workload.</p><p><strong>Fourth stage</strong> The 59% of 192K orphaned OCAIDs with unknown authors need to be checked to see if author information is available from the original records and author / editor data added where possible. Then existing ids can be used to find matches using the methods outlined below. Once all ids and author/title matching attempts are completed, any remaining editions can have works created.</p><p>In the April 2017 data dump there were 4067 duplicate OCAID orphans. In the August 2017 dump there were 7 after the processing and merging by the CleanupBot. These 7 have since been manually merged, so there are no more editions in this class.</p><p>Regenerate statistics for the classes shown in my original graphs for the April data dump. Highlight improvements.</p><p>From the April 2017 graph:</p><table tabindex="0"><thead><tr><th></th><th style="text-align:right;">April</th><th style="text-align:right;">August</th><th style="text-align:right;">Improvement</th></tr></thead><tbody><tr><td>duplicate OCAIDs</td><td style="text-align:right;">130K</td><td style="text-align:right;">128K</td><td style="text-align:right;">2K</td></tr><tr><td>orphaned dupe OCAIDs</td><td style="text-align:right;">4067</td><td style="text-align:right;">0</td><td style="text-align:right;">4K</td></tr><tr><td>other orphaned OCAIDs</td><td style="text-align:right;">219K</td><td style="text-align:right;">192K</td><td style="text-align:right;">27K</td></tr></tbody></table><p><strong>Output</strong> Method to automatically generate the data splits and statistics from any edition dump.</p><h3 id="percentage-of-orphans-with-author-information-august-2017-dump" tabindex="-1">Percentage of orphans with Author information (August 2017 dump) <a class="header-anchor" href="#percentage-of-orphans-with-author-information-august-2017-dump" aria-label="Permalink to &quot;Percentage of orphans with Author information (August 2017 dump)&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Group</th><th>Authors</th></tr></thead><tbody><tr><td>All 25M Editions</td><td>20M editions have author information.</td></tr><tr><td>All 5M Orphans</td><td>1.2M orphans have authors.</td></tr><tr><td>Orphans with OCAIDs (192,800)</td><td>79K orphans with OCAIDs have authors.</td></tr></tbody></table><h3 id="what-were-the-sources-of-orphan-records" tabindex="-1">What were the sources of orphan records? <a class="header-anchor" href="#what-were-the-sources-of-orphan-records" aria-label="Permalink to &quot;What were the sources of orphan records?&quot;">​</a></h3><p><strong>Theory:</strong> these were the early imports and user edits before OL automatically created works. Suspect creation dates to be clustered around 2009. Need to confirm this by collected created_date statistics.</p><p>Sources: from Amazon records? (especially from 2008), from IA MARC records?</p><ul><li>Count and graph sources</li><li>Count and graph created_dates</li></ul><p>... insert created dates graph ...</p><h3 id="operations-to-resolve-orphans" tabindex="-1">Operations to resolve orphans <a class="header-anchor" href="#operations-to-resolve-orphans" aria-label="Permalink to &quot;Operations to resolve orphans&quot;">​</a></h3><ul><li>Delete, if the record is purely bad data</li><li>Redirect, if the edition is an exact duplicate of another edition correctly associated with a work (this happens frequently -- there were often double imports in the early days of OL)</li><li>Associate with existing work</li><li>Create new work and associate, if there are really no existing works or editions</li></ul><p>The Duplicate OCAID orphans had some records that required two orphans to be merged ((2) Redirect above), and then a new work had to be created ((4) Create new work) to resolve the issues. Now that those records have been cleared, only one of the above solutions should be sufficient to resolve any remaining orphaned edition.</p><h3 id="basic-process" tabindex="-1">Basic process <a class="header-anchor" href="#basic-process" aria-label="Permalink to &quot;Basic process&quot;">​</a></h3><p>For each valid orphan we want to determine whether we need to create a new work, or associate it with an existing one.</p><h3 id="merge-technique" tabindex="-1">Merge Technique <a class="header-anchor" href="#merge-technique" aria-label="Permalink to &quot;Merge Technique&quot;">​</a></h3><p>Use unique ids on orphaned editions to find a matching edition already associated with a work,</p><ul><li>if a clear duplicate, convert orphan to a redirect.</li><li>if not a clear duplicate, associate orphan with the work.</li></ul><p>This works well if the original data is good. If the data is bad, incorrect associations can be made. Some bad ids can allow many unrelated editions to become collected under the wrong work. The mitigating factor to this is that the data was already incorrect in the database and needed to be fixed, at least this process groups like with like so the data problem should be more visible and can be eventually fixed in one step.</p><p>Work IDs to use:</p><ul><li>OCAID</li><li>ISBN (with xisbn lookups as an extension) - xisbn is being decomissioned: <a href="https://github.com/xlcnd/isbnlib/issues/51" target="_blank" rel="noreferrer">(problem)</a> <a href="https://github.com/xlcnd/isbntools/issues/96" target="_blank" rel="noreferrer">(possible solution)</a></li><li>other ids: OCLCN, HTID</li></ul><p>Author IDs to use:</p><ul><li>ISNI</li><li>VIAF (careful, these quite often conflate homonymic authors)</li><li>Wikidata Q number</li><li>LCauth</li></ul><h4 id="another-approach" tabindex="-1">Another approach <a class="header-anchor" href="#another-approach" aria-label="Permalink to &quot;Another approach&quot;">​</a></h4><p>For orphans with author information, look for near title matches also by that author to suggest an existing work. Many near misses may vary only by punctuation marks, accents, or by articles &quot;A &quot;, &quot;The &quot;, &quot;Le &quot;, &quot;L&#39;&quot;, &quot;Eine &quot; etc. in variant cataloging of the same edition. Take care not conflate &quot;Part II&quot; vs. &quot;Part III&quot; or similar, which should be treated as different editions of one work. Consider also the possibility of duplicate or conflated author identifiers: &quot;this is not the J. Smith you are looking for&quot;. A fuzzy title match and a fuzzy author match should strongly suggest the same work absent contrary information.</p><h4 id="further-reduction-techniques" tabindex="-1">Further reduction techniques: <a class="header-anchor" href="#further-reduction-techniques" aria-label="Permalink to &quot;Further reduction techniques:&quot;">​</a></h4><p>Identify any orphans that are simply bad data and candidates for deletion. To explore.</p><ul><li>Non-book items</li><li>Blank title, low character title counts</li><li>Garbage strings / spam</li><li>not enough data to be worthwile. What is the threshold?</li></ul><h3 id="synchronising-with-the-ia-v2-availability-api" tabindex="-1">Synchronising with the IA v2 Availability API <a class="header-anchor" href="#synchronising-with-the-ia-v2-availability-api" aria-label="Permalink to &quot;Synchronising with the IA v2 Availability API&quot;">​</a></h3><p>For every previously orphaned OCAID edition that is associated with a work, pass its id to the admin IA sync endpoint.</p>`,65)]))}const m=t(i,[["render",s]]);export{u as __pageData,m as default};
