import{_ as a,c as s,o as i,ag as t}from"./chunks/framework.BRQrZDXk.js";const g=JSON.parse('{"title":"Responding to a Outage","description":"","frontmatter":{},"headers":[],"relativePath":"6_Advanced/Disaster-Recovery-&-Immediate-Response.md","filePath":"6_Advanced/Disaster-Recovery-&-Immediate-Response.md"}'),o={name:"6_Advanced/Disaster-Recovery-&-Immediate-Response.md"};function r(n,e,l,h,p,c){return i(),s("div",null,e[0]||(e[0]=[t(`<p>The Emergency Response Guide for OpenLibrary.org first-responders.</p><h1 id="responding-to-a-outage" tabindex="-1">Responding to a Outage <a class="header-anchor" href="#responding-to-a-outage" aria-label="Permalink to &quot;Responding to a Outage&quot;">​</a></h1><ul><li>[ ] 1. Report outage on <code>#openlibrary</code> and <code>#ops</code> on Slack, follow the <a href="https://docs.google.com/spreadsheets/d/1uu2zQ76TPbx9pP3uSYJ0opldgOHVzlK3Mz35Yk3pUq0/edit?gid=0#gid=0" target="_blank" rel="noreferrer">escalation guide</a></li><li>[ ] 2. ❗ <strong>Search</strong> previous <a href="https://github.com/internetarchive/openlibrary/issues?q=label%3A%22Type%3A+Post-Mortem%22" target="_blank" rel="noreferrer">post mortem</a> reports for insights and solutions to common issues</li><li>[ ] 3. Check the <a href="https://github.com/internetarchive/openlibrary/wiki/Monitoring" target="_blank" rel="noreferrer">public monitoring dashboards</a> and <a href="https://github.com/internetarchive/olsystem/wiki#error-reporting--analytics" target="_blank" rel="noreferrer">internal</a>: <ul><li><a href="https://monitor.archive.org/cgi-bin/nagios3/status.cgi?hostgroup=24.openlibrary&amp;style=detail" target="_blank" rel="noreferrer">NAGIOS</a></li><li><a href="https://openlibrary.org/admin?stats" target="_blank" rel="noreferrer">HAProxy</a></li></ul></li><li>[ ] 4. If the bare-metal machine is hanging, contact #ops on slack or <a href="https://gnt-webmgr.us.archive.org/" target="_blank" rel="noreferrer">manually restart baremetal</a></li><li>[ ] 5. If there&#39;s a fiber outage and openlibrary.org&#39;s servers don&#39;t resolve (even to Sorry service), ask in the internal slack channels <code>#openlibrary</code> or <code>#ops</code> for openlibrary.org to be temporarily pointed to an active &quot;Sorry Server&quot;</li><li>[ ] 6. Create a new <a href="https://github.com/internetarchive/openlibrary/issues/new?assignees=&amp;labels=Type%3A+Post-Mortem%2C+Priority%3A+0%2C+GJ%3A+Triage+Exception&amp;template=post_mortem.md&amp;title=" target="_blank" rel="noreferrer">postmortem</a> issue and <em>proceed to this guide</em>:</li></ul><h1 id="diagnostic-s-guide" tabindex="-1">Diagnostic&#39;s Guide <a class="header-anchor" href="#diagnostic-s-guide" aria-label="Permalink to &quot;Diagnostic&#39;s Guide&quot;">​</a></h1><p>Before continuing, check our <a href="https://docs.google.com/document/d/1QQuQtEjTP2V5OuoU13tTKTlB2cypq76uP4IdSeTRQjA/edit?tab=t.0" target="_blank" rel="noreferrer">Performance Monitoring Guide</a> <a href="https://github.com/internetarchive/openlibrary/issues?q=is%3Aissue+label%3A%22Type%3A+Post-Mortem%22+" target="_blank" rel="noreferrer">Port-mortems</a> to see if this is a known / already solved problem.</p><ol><li>Is <a href="https://grafana.us.archive.org/d/b7a222a0-d4fe-49a4-a5c4-b071ce756fda/ol-cluster-load?orgId=1&amp;refresh=1m" target="_blank" rel="noreferrer">CPU load high on web nodes</a> and/or is there a <a href="https://sentry.archive.org/organizations/ia-ux/alerts/rules/details/23/" target="_blank" rel="noreferrer">spike in # of transactions</a>? <ul><li><a href="https://github.com/internetarchive/openlibrary/wiki/Disaster-Recovery-&amp;-Immediate-Response#handling-abuse--ddos-denial-of-service-attack" target="_blank" rel="noreferrer">Handling Abuse &amp; DDOS (Denial of Service Attack)</a></li></ul></li><li>Are <code>ol-mem*</code> slow to ssh into? We may want to <code>/etc/init.d/memcached restart</code> or even <a href="https://gnt-webmgr.us.archive.org/cluster/cluster1#virtual_machines" target="_blank" rel="noreferrer">manually restart bare-metal</a> if ssh hangs for more than 3 minutes</li><li>Does <a href="#Homepage_Errors">homepage cache</a> look weird?</li></ol><h2 id="spam" tabindex="-1">Spam <a class="header-anchor" href="#spam" aria-label="Permalink to &quot;Spam&quot;">​</a></h2><ol><li>There is an admin dashboard for blocking certain terms from appearing on Open Library: <a href="https://openlibrary.org/admin/spamword" target="_blank" rel="noreferrer">https://openlibrary.org/admin/spamword</a></li><li>You can also block &amp; revert changes per specific accounts via <a href="https://openlibrary.org/admin/people" target="_blank" rel="noreferrer">https://openlibrary.org/admin/people</a></li></ol><ul><li>If the edit to a page contains any of the spam words or email of the user is from the blacklisted domains, the edit won’t be accepted. New registrations with emails from those domains are also not accepted.</li></ul><h2 id="power-outages-at-data-center" tabindex="-1">Power Outages at Data Center <a class="header-anchor" href="#power-outages-at-data-center" aria-label="Permalink to &quot;Power Outages at Data Center&quot;">​</a></h2><p>Once services return, please make sure all services are running and that VMs are ssh&#39;able (this can probably be a script).</p><p>If a machine is up but not reachable, <a href="https://gnt-webmgr.us.archive.org/" target="_blank" rel="noreferrer">manually restart baremetal</a>. If a machine is up and reachable but services are not running, check <code>docker ps</code> on the host.</p><h2 id="handling-abuse-ddos-denial-of-service-attack" tabindex="-1">Handling Abuse &amp; DDOS (Denial of Service Attack) <a class="header-anchor" href="#handling-abuse-ddos-denial-of-service-attack" aria-label="Permalink to &quot;Handling Abuse &amp; DDOS (Denial of Service Attack)&quot;">​</a></h2><p>We have a few graphs on our main dashboard to <a href="https://grafana.us.archive.org/d/000000176/open-library-dev?orgId=1&amp;refresh=1m&amp;from=now-24h&amp;to=now" target="_blank" rel="noreferrer">monitor the traffic of the top requesting IPs</a> to observe changes in pattern/behaviour:</p><p><img src="https://github.com/internetarchive/openlibrary/assets/6251786/d4c6a6ed-0a48-47b4-a9bc-cf1f8c7b6ee5" alt="image"></p><p>These graphs show the ratio between the last ~20k requests across the top IPs hitting our website. E.g. the green section in the first graph, what ratio of requests came from the top IP. The yellow is from the second top-most IP. And so on.</p><p>In the graph above, you can see several anomalies where the top IP has been making significantly more requests. This is an indicator that there might be abuse happening from a certain IP.</p><p>Treatment: Investigate and block the IP as necessary.</p><ol><li>Investigate the traffic to verify. On the server in question (eg <code>ssh -A ol-www0</code>):</li></ol><div class="language-sh vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">sh</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Observe the recent requests hitting the server. Note 150000 is largely arbitrary.</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">$</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> tail</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -n</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 150000</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> /1/var/log/nginx/access.log</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> |</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> grep</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -oE</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;^[0-9.()]+&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> |</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> sort</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> |</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> uniq</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -c</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> |</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> sort</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -rn</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> |</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> head</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -n</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 25</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 154598 0.32.37.207</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 125985 0.3.111.38</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 124110 0.45.210.121</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 123793 0.249.158.151</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 122969 0.79.152.249</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 122872 0.244.113.216</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 122526 0.30.143.17</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 121269 0.145.106.249</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 120520 0.85.80.58</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 117442 0.141.6.36</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 109663 0.204.1.42</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">#  90027 0.109.144.144</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">#  81801 0.218.22.254</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">#  ...</span></span></code></pre></div><p>You can see the top IP (note the IPs are anonymoized) is causing a considerable amount of traffic. Let&#39;s investigate it.</p><div class="language-sh vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">sh</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">$</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> tail</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> /1/var/log/nginx/access.log</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> |</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> grep</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -F</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;0.32.37.207&#39;</span></span></code></pre></div><p>This will let you see the traffic from that IP and determine if it should be blocked. Use your discretion to check any given IP to see whether the pattern looks abusive / spammy -- e.g. Internet Archive makes many requests to /api/books.json and still we don&#39;t want to ban it, for example. If you determine it should be blocked, then we need to get the denanoymized IP and add it to our deny.conf.</p><p>On <code>ol-www0</code> in <code>/openlibrary</code> you can run <code>decode_ip.sh</code> with the offending anonymized IP <code>0.32.37.207</code> as follows:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>cd /opt/openlibrary/scripts</span></span>
<span class="line"><span>sudo -E SEED_PATH=http://seedserver.us.archive.org/key/seed.txt ./decode_ip.sh 0.32.37.207</span></span></code></pre></div><p>Note: if you run <code>decode.sh</code> and get a file not found error, run it again. This is a work around until a fix is merged for a race condition around the creation of the IP map.</p><h2 id="solr-search-issues" tabindex="-1">Solr Search Issues <a class="header-anchor" href="#solr-search-issues" aria-label="Permalink to &quot;Solr Search Issues&quot;">​</a></h2><p>You can restart solr via docker as:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>ssh -A ol-solr1</span></span>
<span class="line"><span>docker restart solr_builder_solr_1 solr_builder_haproxy_1</span></span></code></pre></div><h1 id="out-of-space" tabindex="-1">Out of Space <a class="header-anchor" href="#out-of-space" aria-label="Permalink to &quot;Out of Space&quot;">​</a></h1><h2 id="cleanup-deploys" tabindex="-1">Cleanup Deploys <a class="header-anchor" href="#cleanup-deploys" aria-label="Permalink to &quot;Cleanup Deploys&quot;">​</a></h2><p>There are few servers which we expect to fill up. ol-db1/2 and ol-covers0/1 are candidates because their job is to store temporary or long term data. ol-home0 is another service which generates data dumps, aggregates partner data, and generates sitemaps. These three servers likely need a manual investigation when nagios reports their space is low.</p><p>The following will prune unattached images which were created more than 1 week ago (168h):</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># to prune the build cache</span></span>
<span class="line"><span>docker builder prune</span></span>
<span class="line"><span></span></span>
<span class="line"><span># prune unused images created more than 1 week ago</span></span>
<span class="line"><span>docker image prune -a --filter &quot;until=168h&quot;</span></span></code></pre></div><div class="caution custom-block github-alert"><p class="custom-block-title">CAUTION</p><p>When <code>docker prune</code> is being run, unfortunately the rest of <code>docker</code> typically becomes unresponsive; <a href="https://github.com/docker/for-mac/issues/2501" target="_blank" rel="noreferrer">see this issue</a>. When this happens, the <em>wrong</em> intervention is to try and restart the server with ganeti. When you do, not only will docker still be unresponsive until the prune finishes, but <em>additionally</em> all docker containers that <em>were</em> running will stop and be unreachable.</p></div><h2 id="docker-images" tabindex="-1">Docker images <a class="header-anchor" href="#docker-images" aria-label="Permalink to &quot;Docker images&quot;">​</a></h2><p>Even with this being the case, a very common cause of disk fill are out docker images which have not been pruned during our deploy process. These can be many GB over time. Run <code>docker image ls</code> for a listing of images registered in docker to see if any of them can be pruned or deleted.</p><h2 id="docker-logs" tabindex="-1">Docker Logs <a class="header-anchor" href="#docker-logs" aria-label="Permalink to &quot;Docker Logs&quot;">​</a></h2><p>Docker logs can take up a ton of space. @cdrini mentions one solution is: (Truncating docker logs for container with ID d12b...)</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>sudo df -h - See the sizes of a bunch of things on the VM</span></span>
<span class="line"><span>truncate -s 0 $(docker inspect --format=&#39;{{.LogPath}}&#39; d12b518475e1)</span></span></code></pre></div><h2 id="ol-dev1-out-of-storage" tabindex="-1">ol-dev1 out of storage <a class="header-anchor" href="#ol-dev1-out-of-storage" aria-label="Permalink to &quot;ol-dev1 out of storage&quot;">​</a></h2><p>Symptom: <code>sudo df -h</code> shows a bunch of <code>100%</code> or <code>99%</code>. Testing deploys might fail on occasion.</p><p>Containers and images can stick around on our dev server causing it to fill up. To free up space:</p><ol><li>Confirm with folks on slack, #team-abc, that there are not stopped containers that people care about. There shouldn&#39;t be. There is some risk of data loss if someone has made modification to the file system inside a now stopped container. That is why we confirm!</li><li>Run <code>docker container prune</code></li><li>Run <code>docker images prune</code> . This will remove any images; all images should have <code>Dockerfiles</code> somewhere, so there&#39;s little risk of data loss. But it might be annoying because someone will have to rebuild a docker image they might care about and have to find the <code>Dockerfile</code>!</li></ol><h2 id="upstart-log" tabindex="-1">upstart.log <a class="header-anchor" href="#upstart-log" aria-label="Permalink to &quot;upstart.log&quot;">​</a></h2><p>There is a possibility supervisor can get confused (perhaps related to permissions/<code>chown</code>), and instead of rotating logs, will start writing to <code>/var/log/openlibrary/upstart.log</code> until <code>/dev/vda1</code> (or wherever root / is mounted) runs out of space. The solution is to restart &quot;supervisor&quot; (not openlibrary via supervistorctl but supervisor itself) on the aflicted node (e.g. <code>ol-web4</code> in this example):</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>sudo service supervisor restart</span></span></code></pre></div><p>If successful, you should see a new openlibrary.log with an update time more recent than upstart.log. One you&#39;ve confirmed this, you can truncate the erroneously inflated upstart.log to free up disk space:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>sudo truncate upstart.log  --size 0</span></span></code></pre></div><p>After truncating, you&#39;ll want to restart <code>openlibrary</code>, e.g.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>ssh ol-web4 sudo supervisorctl restart openlibrary</span></span></code></pre></div><h1 id="homepage-errors" tabindex="-1">Homepage Errors <a class="header-anchor" href="#homepage-errors" aria-label="Permalink to &quot;Homepage Errors&quot;">​</a></h1><p>Sometimes an error occurs while compiling the homepage and an empty body is cached: <a href="https://github.com/internetarchive/openlibrary/issues/6646" target="_blank" rel="noreferrer">https://github.com/internetarchive/openlibrary/issues/6646</a></p><p><em>Solution:</em> You can use this the url to hit to clear the homepage memcache entry: <a href="https://openlibrary.org/admin/inspect/memcache?keys=home.homepage.en.pd-&amp;action=delete" target="_blank" rel="noreferrer">https://openlibrary.org/admin/inspect/memcache?keys=home.homepage.en.pd-&amp;action=delete</a> . Note the .pd . Remove that if you want to clear the cache for non printdisabled users.</p><h2 id="notes" tabindex="-1">Notes <a class="header-anchor" href="#notes" aria-label="Permalink to &quot;Notes&quot;">​</a></h2><ul><li>If solr-updater or import-bot or deploy issue, or infobase (API), check <code>ol-home</code></li><li>If lending information e.g. books appear as available on OL when they are waitlisted on IA, this is a freak incident w/ memcached and we&#39;ll need to ssh into each memcached (ol-mem*) and <code>sudo service memcached restart</code></li><li>If there&#39;s an issue with ssl termination, static assets, connecting to the website, check <code>ol-www1</code> (which is where all traffic enters and goes into haproxy -- which also lives on this machine). Another case is abuse, which is documented in the troubleshooting guide (usually haproxy limits or banning via nginx <code>/opt/openlibrary/olsystem/etc/nginx/deny.conf</code></li><li>If there&#39;s a database problem, sorry (<code>ol-db0</code> primary, <code>ol-db1</code> replication, <code>ol-backup1</code>)</li><li>If we&#39;re seeing <code>ol-web1</code> and <code>ol-web2</code> offline, it may be network, upstream, DNS, or a breaking dependency, CHECK <a href="https://monitor.archive.org/cgi-bin/nagios3/status.cgi?hostgroup=24.openlibrary&amp;style=detail" target="_blank" rel="noreferrer">NAGIOS</a> + alert #ops + #openlibrary. Check the logs in <code>/var/log/openlibrary/</code> (esp. <code>upstart.log</code>)</li><li>If you notice a disk filling up rapidly or almost out of space... CREATE A BASILISK FILE (an empty 2GB placeholder <code>dd</code>&#39;d file that we can delete and have the ability to <code>ls</code>, etc)</li></ul><ul><li><a href="#Handling_Server_Reboot">Is the server having trouble after rebooting?</a></li><li><a href="#Handling_DDOS">Is OpenLibrary getting slammed with traffic, crawlers, or bad actors?</a></li><li><a href="#Overloaded_Search">Is Search Overloading archive.org elastic search upstream?</a></li></ul>`,57)]))}const u=a(o,[["render",r]]);export{g as __pageData,u as default};
